# LLM-Powered Lecture Comprehension Assistant

**NYU 25 Spring Â· CSCI-GA.3033-102 - Learning with Large Language and Vision Models**  
**Saining Xie**

*Group Project*

![App Design](./img/pipeline.png)


## ðŸ“Œ Project Description

**LLM-Powered Lecture Comprehension Assistant** is a multimodal system that helps students efficiently review and understand lecture content through automatic annotation.

Users upload a pair of lecture materials â€” an **audio recording** and a **slide image** â€” and receive powerful annotations generated by a language model. The workflow integrates:

- **Whisper** API: Transcribes the uploaded **MP3** audio into plain text
- **LLaVA** (from Hugging Face): Processes both the slide image (**picture**) and transcribed audio (**text**) 
- **vLLM**: Hosts LLaVA with an OpenAI-compatible interface for fast, local inference
- **FastAPI + html**: Powers a lightweight web server with a clean frontend for file upload and result display

This system combines **speech recognition**, **vision-language modeling**, and a **web interface** to enable smart, accessible lecture summarization and comprehension â€” all in the browser.

## ðŸ‘¥ Contributors

- **Changyue Su** [@SUcy6]  
- **Ziyun Cheng** [@Zoey-Cheng]  
- **Lin Yang** [@molliey]  
- **Kai Zhong** [@ZhongKai-07]  

Emails: `{cs7483, zc3068, ly2431, kz2673}@nyu.edu`

## ðŸ”§ Usage

### Part 1. LLaVA Inference with vLLM

We use **vLLM** to host [LLaVA 1.5-7B](https://huggingface.co/llava-hf/llava-1.5-7b-hf) for local, OpenAI-style multimodal inference. Tested on **A100 40GB**.

#### Setup vLLM Environment

Install [`vLLM`](https://docs.vllm.ai/en/latest/getting_started/quickstart.html) using [`uv`](https://github.com/astral-sh/uv), a modern Python package manager:

```
# Install uv (a faster Python package manager)
curl -LsSf https://astral.sh/uv/install.sh | sh

# Create Python 3.10 virtual environment
uv venv myenv --python 3.10 --seed
source myenv/bin/activate

# Install vLLM
uv pip install vllm
```

#### Authenticate Hugging Face Access

You'll need a [Hugging Face access token](https://huggingface.co/docs/hub/en/security-tokens) to download the model:

```
export HF_TOKEN=your_hf_token_here

# Verify login
huggingface-cli whoami
```

#### Launch vLLM Server with LLaVA

Start the OpenAI-compatible API server using `llava-hf/llava-1.5-7b-hf`:

```
python3 -m vllm.entrypoints.openai.api_server \
  --model llava-hf/llava-1.5-7b-hf \
  --host 0.0.0.0 \
  --port 8000 \
  --dtype float16
```

------

### Part 2. Launch the Web Application

Our web app uses:

- **Whisper** to transcribe lecture audio
- **LLaVA via vLLM** for multimodal annotation

To run the FastAPI-based web interface:

```
cd llvm-project
pip install -r requirement.txt

uvicorn app:app --host 0.0.0.0 --port 8080
```
Please make sure that you have inbound rules for port 8080

Then visit:
 ðŸ‘‰ `http://<your-cloud-instance-public-ip>:8080` in your browser.


 ## ðŸŽ¦ Demo

 demo video could be found on: [`https://youtu.be/10-4UdWIcpo`](https://youtu.be/10-4UdWIcpo)
